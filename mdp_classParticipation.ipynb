{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MDP:\n",
    "\n",
    "    \"\"\"A Markov Decision Process, defined by an initial state, transition model,\n",
    "    and reward function. We also keep track of a gamma value, for use by\n",
    "    algorithms. The transition model is represented somewhat differently from\n",
    "    the text. Instead of P(s' | s, a) being a probability number for each\n",
    "    state/state/action triplet, we instead have T(s, a) return a\n",
    "    list of (p, s') pairs. We also keep track of the possible states,\n",
    "    terminal states, and actions for each state.\"\"\"\n",
    "\n",
    "    def __init__(self, init, actlist, terminals, transitions = {}, reward = None, states=None, gamma=.9):\n",
    "        if not (0 < gamma <= 1):\n",
    "            raise ValueError(\"An MDP must have 0 < gamma <= 1\")\n",
    "\n",
    "        if states:\n",
    "            self.states = states\n",
    "        else:\n",
    "            ## collect states from transitions table\n",
    "            self.states = self.get_states_from_transitions(transitions)\n",
    "            \n",
    "        \n",
    "        self.init = init\n",
    "        \n",
    "        if isinstance(actlist, list):\n",
    "            ## if actlist is a list, all states have the same actions\n",
    "            self.actlist = actlist\n",
    "        elif isinstance(actlist, dict):\n",
    "            ## if actlist is a dict, different actions for each state\n",
    "            self.actlist = actlist\n",
    "        \n",
    "        self.terminals = terminals\n",
    "        self.transitions = transitions\n",
    "        if self.transitions == {}:\n",
    "            print(\"Warning: Transition table is empty.\")\n",
    "        self.gamma = gamma\n",
    "        if reward:\n",
    "            self.reward = reward\n",
    "        else:\n",
    "            self.reward = {s : 0 for s in self.states}\n",
    "        #self.check_consistency()\n",
    "\n",
    "    def R(self, state):\n",
    "        \"\"\"Return a numeric reward for this state.\"\"\"\n",
    "        return self.reward[state]\n",
    "\n",
    "    def T(self, state, action):\n",
    "        \"\"\"Transition model. From a state and an action, return a list\n",
    "        of (probability, result-state) pairs.\"\"\"\n",
    "        if(self.transitions == {}):\n",
    "            raise ValueError(\"Transition model is missing\")\n",
    "        else:\n",
    "            return self.transitions[state][action]\n",
    "\n",
    "    def actions(self, state):\n",
    "        \"\"\"Set of actions that can be performed in this state. By default, a\n",
    "        fixed list of actions, except for terminal states. Override this\n",
    "        method if you need to specialize by state.\"\"\"\n",
    "        if state in self.terminals:\n",
    "            return [None]\n",
    "        else:\n",
    "            return self.actlist\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions:  {1: [2], 2: [1, 3, 4, 5], 3: [0], 4: [0], 5: [2, 6, 7, 8], 6: [0], 7: [0], 8: [5, 9, 10, 11], 9: [0], 10: [0], 11: [0]}\n"
     ]
    }
   ],
   "source": [
    "states = [1,2,3,4,5,6,7,8,9,10,11]\n",
    "terminals = [3,4,6,7,9,10,11]\n",
    "nonterminal = [1,2,5,8]\n",
    "prob = dict()\n",
    "actions = dict([(s,[0]) for s in states])\n",
    "actions[1] = [2]\n",
    "actions[2] = [1,3,4,5]\n",
    "actions[5] = [2,6,7,8]\n",
    "actions[8] = [5,9,10,11]\n",
    "print(\"Actions: \", actions)\n",
    "rewards_terminal = [(3, 2), (4, 2), (6, 3), (7, 3), (9, -10), (10, -10), (11, 10)]\n",
    "def T(state,action, p):\n",
    "    prob[(state,action)] = p\n",
    "\n",
    "T(1,2,[(1,0.1), (2,0.9)])\n",
    "T(2,1,[(1, 0.7), (2, 0.1), (3, 0.1), (4, 0.1)])\n",
    "T(2,3,[(1, 0.1), (2, 0.1), (3, 0.6), (4, 0.1), (5, 0.1)])\n",
    "T(2,4,[(1, 0.1), (2, 0.1), (3, 0.1), (4, 0.6), (5, 0.1)])\n",
    "T(2,5,[(1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.6)])\n",
    "T(5,2,[(2, 0.6), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1)])\n",
    "T(5,6,[(2, 0.1), (5, 0.1), (6, 0.6), (7, 0.1), (8, 0.1)])\n",
    "T(5,7,[(2, 0.1), (5, 0.1), (6, 0.1), (7, 0.6), (8, 0.1)])\n",
    "T(5,8,[(2, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.6)])\n",
    "T(8,5,[(5, 0.6), (8, 0.1), (9, 0.1), (10, 0.1), (11, 0.1)])\n",
    "T(8,9,[(5, 0.1), (8, 0.1), (9, 0.6), (10, 0.1), (11, 0.1)])\n",
    "T(8,10,[(5, 0.1), (8, 0.1), (9, 0.1), (10, 0.6), (11, 0.1)])\n",
    "T(8,11,[(5, 0.1), (8, 0.1), (9, 0.1), (10, 0.1), (11, 0.6)])\n",
    "init = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Terminal States:  [3, 4, 6, 7, 9, 10, 11]\n",
      "Actions:  {1: [2], 2: [1, 3, 4, 5], 3: [0], 4: [0], 5: [2, 6, 7, 8], 6: [0], 7: [0], 8: [5, 9, 10, 11], 9: [0], 10: [0], 11: [0]}\n",
      "Tranistion Matrix:  {(1, 2): [(1, 0.1), (2, 0.9)], (2, 1): [(1, 0.7), (2, 0.1), (3, 0.1), (4, 0.1)], (2, 3): [(1, 0.1), (2, 0.1), (3, 0.6), (4, 0.1), (5, 0.1)], (2, 4): [(1, 0.1), (2, 0.1), (3, 0.1), (4, 0.6), (5, 0.1)], (2, 5): [(1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.6)], (5, 2): [(2, 0.6), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1)], (5, 6): [(2, 0.1), (5, 0.1), (6, 0.6), (7, 0.1), (8, 0.1)], (5, 7): [(2, 0.1), (5, 0.1), (6, 0.1), (7, 0.6), (8, 0.1)], (5, 8): [(2, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.6)], (8, 5): [(5, 0.6), (8, 0.1), (9, 0.1), (10, 0.1), (11, 0.1)], (8, 9): [(5, 0.1), (8, 0.1), (9, 0.6), (10, 0.1), (11, 0.1)], (8, 10): [(5, 0.1), (8, 0.1), (9, 0.1), (10, 0.6), (11, 0.1)], (8, 11): [(5, 0.1), (8, 0.1), (9, 0.1), (10, 0.1), (11, 0.6)]}\n"
     ]
    }
   ],
   "source": [
    "print(\"States: \", states)\n",
    "print(\"Terminal States: \", terminals)\n",
    "print(\"Actions: \", actions)\n",
    "print(\"Tranistion Matrix: \", prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_iteration(mdp, epsilon=0.001):\n",
    "    \n",
    "    U1 = {s: 0 for s in mdp.states}\n",
    "    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
    "    while True:\n",
    "        U = U1.copy()\n",
    "        delta = 0\n",
    "        for s in mdp.states:\n",
    "            U1[s] = R(s) + gamma * max([sum([p * U[s1] for (p, s1) in T(s, a)])\n",
    "                                        for a in CustomMDP.actions[s]])\n",
    "            delta = max(delta, abs(U1[s] - U[s]))\n",
    "        if delta < epsilon * (1 - gamma) / gamma:\n",
    "            return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setR(reward):\n",
    "    rewards_nonterminal = []\n",
    "    for s in nonterminal:\n",
    "        rewards_nonterminal.append((s,reward))\n",
    "    rewards = rewards_terminal + rewards_nonterminal\n",
    "    return rewards\n",
    "            \n",
    "def getR(state):\n",
    "    for r in rewards:\n",
    "        if r[0] == state:\n",
    "            return r[1]\n",
    "\n",
    "\n",
    "def Value(s,a,U):\n",
    "    total = 0\n",
    "    if a != 0:\n",
    "        for p in prob[(s,a)]:\n",
    "            total += p[1] * U[p[0]]\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_iteration(gamma):\n",
    "    \"Solving an MDP by value iteration.\"\n",
    "    epsilon=0.01\n",
    "    U1 = dict([(s, 0) for s in states])\n",
    "    count = 0\n",
    "    while True:\n",
    "        U = U1.copy()\n",
    "        delta = 0\n",
    "        for s in states:\n",
    "            l = []\n",
    "            for a in actions[s]:\n",
    "                l.append(Value(s,a,U))\n",
    "            if len(l) > 0:\n",
    "                m = max(l)\n",
    "            else:\n",
    "                m = 0\n",
    "            U1[s] = getR(s) + gamma*m\n",
    "            U1[s] = round(U1[s],4)\n",
    "            delta = max(delta, abs(U1[s] - U[s]))\n",
    "        count += 1\n",
    "        #print(U)\n",
    "        if delta <= epsilon * (1 - gamma) / gamma:\n",
    "            #print(\"Number of iterations: \", count-1)\n",
    "            return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-terminal reward:  -0.1\n",
      "Rewards:  [(3, 2), (4, 2), (6, 3), (7, 3), (9, -10), (10, -10), (11, 10), (1, -0.1), (2, -0.1), (5, -0.1), (8, -0.1)]\n",
      "Discounting:  1\n",
      "Utility:  {1: 3.3516, 2: 3.4627, 3: 2, 4: 2, 5: 4.1355, 6: 3, 7: 3, 8: 4.7928, 9: -10, 10: -10, 11: 10}\n"
     ]
    }
   ],
   "source": [
    "reward = -0.1\n",
    "print(\"Non-terminal reward: \", reward)\n",
    "rewards = setR(reward)\n",
    "print(\"Rewards: \", rewards)\n",
    "gamma = 1\n",
    "print(\"Discounting: \", gamma)\n",
    "utility = value_iteration(gamma = 1)\n",
    "print(\"Utility: \",utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-terminal reward:  -0.7\n",
      "Rewards:  [(3, 2), (4, 2), (6, 3), (7, 3), (9, -10), (10, -10), (11, 10), (1, -0.7), (2, -0.7), (5, -0.7), (8, -0.7)]\n",
      "Discounting:  0.8\n",
      "Utility:  {1: -0.3354, 2: 0.5451, 3: 2.0, 4: 2.0, 5: 1.359, 6: 3.0, 7: 3.0, 8: 2.8355, 9: -10.0, 10: -10.0, 11: 10.0}\n"
     ]
    }
   ],
   "source": [
    "reward = -0.7\n",
    "print(\"Non-terminal reward: \", reward)\n",
    "rewards = setR(reward)\n",
    "print(\"Rewards: \", rewards)\n",
    "gamma = 0.8\n",
    "print(\"Discounting: \", gamma)\n",
    "utility = value_iteration(gamma = 0.8)\n",
    "print(\"Utility: \",utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-terminal reward:  -0.1\n",
      "Rewards:  [(3, 2), (4, 2), (6, 3), (7, 3), (9, -10), (10, -10), (11, 10), (1, -0.1), (2, -0.1), (5, -0.1), (8, -0.1)]\n",
      "Discounting:  0.8\n",
      "Utility:  {1: 1.1414, 2: 1.5999, 3: 2.0, 4: 2.0, 5: 2.4196, 6: 3.0, 7: 3.0, 8: 3.5799, 9: -10.0, 10: -10.0, 11: 10.0}\n"
     ]
    }
   ],
   "source": [
    "reward = -0.1\n",
    "print(\"Non-terminal reward: \", reward)\n",
    "rewards = setR(reward)\n",
    "print(\"Rewards: \", rewards)\n",
    "gamma = 0.8\n",
    "print(\"Discounting: \", gamma)\n",
    "utility = value_iteration(gamma = 0.8)\n",
    "print(\"Utility: \",utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pi = best_policy(mdp, value_iteration(mdp, .01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimal_policy(U):\n",
    "    pi = {}\n",
    "    for s in states:\n",
    "        l = []\n",
    "        for a in actions[s]:\n",
    "            l.append((a,Value(s,a,U)))\n",
    "        if len(l) > 0 :\n",
    "            pi[s] = max(l,key=lambda item:item[1])[0]\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_evaluation(policy,U,gamma,k=20):\n",
    "    for i in range(k):\n",
    "        for s in states:\n",
    "            U[s] = getR(s) + gamma*Value(s,policy[s],U)\n",
    "    print(U)\n",
    "    return U\n",
    "    \n",
    "def policy_iteration(policy,gamma):\n",
    "    U = dict([(s, 0) for s in states])\n",
    "    while True:\n",
    "        U = policy_evaluation(policy,U,gamma)\n",
    "        #print(U)\n",
    "        unchanged = True\n",
    "        for s in states:\n",
    "            l = []\n",
    "            for a in actions[s]:\n",
    "                if a != 0: \n",
    "                    l.append((a,Value(s,a,U)))\n",
    "            if len(l) > 0:\n",
    "                m = max(l,key=lambda item:item[1])[1]\n",
    "            if m > Value(s,policy[s],U):\n",
    "                if len(l) > 0:\n",
    "                    policy[s] = max(l,key=lambda item:item[1])[0]\n",
    "                    unchanged = False\n",
    "        if unchanged:\n",
    "            #U = [round(U[s],4) for s in states]\n",
    "            print(U)\n",
    "            return policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy:  {1: 2, 2: 5, 3: 0, 4: 0, 5: 8, 6: 0, 7: 0, 8: 11, 9: 0, 10: 0, 11: 0}\n",
      "\n",
      " Policy iteration\n",
      "Given Policy:  {1: 2, 2: 1, 5: 2, 8: 5, 3: 0, 4: 0, 6: 0, 7: 0, 9: 0, 10: 0, 11: 0}\n",
      "{1: 0.14981111191040095, 2: 0.3303194191265836, 3: 2.0, 4: 2.0, 5: 0.5240949594599779, 6: 3.0, 7: 3.0, 8: -0.7048201563646231, 9: -10.0, 10: -10.0, 11: 10.0}\n",
      "{1: 0.9711521606175738, 2: 1.3798055385670014, 3: 2.0, 4: 2.0, 5: 2.1466115329030124, 6: 3.0, 7: 3.0, 8: 3.5562270898176536, 9: -10.0, 10: -10.0, 11: 10.0}\n",
      "{1: 1.1445153617245039, 2: 1.6013251844289011, 3: 2.0, 4: 2.0, 5: 2.4201207098687108, 6: 3.0, 7: 3.0, 8: 3.5800104965103188, 9: -10.0, 10: -10.0, 11: 10.0}\n",
      "{1: 1.1445153617245039, 2: 1.6013251844289011, 3: 2.0, 4: 2.0, 5: 2.4201207098687108, 6: 3.0, 7: 3.0, 8: 3.5800104965103188, 9: -10.0, 10: -10.0, 11: 10.0}\n",
      "Optimal Policy:  {1: 2, 2: 5, 5: 8, 8: 11, 3: 0, 4: 0, 6: 0, 7: 0, 9: 0, 10: 0, 11: 0}\n"
     ]
    }
   ],
   "source": [
    "best_policy = optimal_policy(utility)\n",
    "print(\"Optimal Policy: \", best_policy)\n",
    "\n",
    "print(\"\\n Policy iteration\")\n",
    "policy ={1: 2, 2: 1, 5: 2, 8: 5, 3: 0, 4: 0, 6: 0, 7: 0, 9: 0, 10: 0, 11: 0}\n",
    "print(\"Given Policy: \",policy)\n",
    "p = policy_iteration(policy,gamma)\n",
    "print(\"Optimal Policy: \",p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
